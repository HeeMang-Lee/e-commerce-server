# 장애 대응 보고서 (Post-Incident Analysis)

> 이 문서는 부하 테스트 중 발생한 장애 상황을 실제 프로덕션 장애로 가정하여 작성한 **가상의 포스트 모템**입니다.
> GitHub, LINE Engineering의 장애 보고 문화를 참고하여 작성되었습니다.

---

# 장애 보고서 #1: 블랙프라이데이 주문 폭주로 인한 서버 다운

## 1. 사건 개요

### 1.1 요약

| 항목 | 내용 |
|------|------|
| **장애 등급** | P1 (Critical) |
| **발생 일시** | 2025-12-26 09:00 KST |
| **복구 일시** | 2025-12-26 09:47 KST |
| **총 장애 시간** | 47분 |
| **영향 서비스** | 전체 API (주문, 결제, 상품 조회) |
| **영향 사용자** | 약 15,000명 |

### 1.2 한 줄 요약

블랙프라이데이 세일 시작과 동시에 트래픽이 평소 대비 15배 폭증하여 Tomcat 스레드 풀과 DB 커넥션 풀이 고갈되었고, 이로 인해 서버가 신규 연결을 거부하면서 전체 서비스가 47분간 중단되었다.

---

## 2. 타임라인

모든 시간은 KST(한국 표준시) 기준입니다.

```
2025-12-26

08:55  블랙프라이데이 세일 페이지 오픈 (예정대로)
       - 사전 예고로 대기 트래픽 발생 시작

09:00  세일 시작, 동시 접속자 급증
       ├─ TPS: 50 → 450 (9배 증가)
       └─ 동시 접속: 200 → 2,800명

09:03  첫 번째 경고 알림 발생
       ├─ [WARNING] CPU 사용률 78%
       ├─ [WARNING] Tomcat 활성 스레드 180/200
       └─ 담당자 A, 알림 확인 후 모니터링 시작

09:07  Tomcat 스레드 풀 고갈
       ├─ [CRITICAL] Tomcat 활성 스레드 200/200 (100%)
       ├─ 신규 요청 대기열(accept-count) 누적 시작
       └─ 응답 시간 P95: 150ms → 3,200ms

09:09  HikariCP 커넥션 풀 고갈
       ├─ [CRITICAL] DB 커넥션 10/10 (100%)
       ├─ "Connection is not available" 에러 로그 급증
       └─ 담당자 B, 장애 상황 선언

09:11  서버 응답 불가 시작
       ├─ [CRITICAL] accept-count 100 초과
       ├─ 클라이언트 측 "connection reset by peer" 에러
       ├─ 전체 API 응답 불가
       └─ 에러율: 0% → 89%

09:12  장애 대응 시작
       ├─ 담당자 A: 서버 로그 분석
       ├─ 담당자 B: 인프라 메트릭 확인
       └─ 담당자 C: 고객 공지 준비

09:15  1차 조치: 서버 재시작 시도
       ├─ 재시작 완료
       └─ 즉시 재발 (트래픽 여전히 높음)

09:20  2차 조치: 트래픽 제한 결정
       ├─ Nginx rate limiting 임시 적용 (100 req/s)
       └─ 일부 요청 처리 시작, 에러율 89% → 45%

09:25  고객 공지 발송
       └─ "일시적 접속 장애 안내, 순차 접속 부탁"

09:30  3차 조치: 설정 튜닝 적용
       ├─ Tomcat max-threads: 200 → 400
       ├─ HikariCP pool-size: 10 → 50
       └─ 서버 재시작

09:40  서비스 점진적 복구
       ├─ 에러율: 45% → 5%
       └─ TPS: 350 req/s 안정화

09:47  서비스 정상화 확인
       ├─ 에러율: 0.3%
       ├─ P95 응답시간: 280ms
       └─ 장애 종료 선언

10:30  rate limiting 단계적 해제
       └─ 100 → 300 → 무제한

11:00  사후 분석 회의 시작
```

---

## 3. 근본 원인 분석 (Root Cause Analysis)

### 3.1 직접 원인

**Tomcat 스레드 풀(200개)과 HikariCP 커넥션 풀(10개)이 트래픽 폭증을 감당하지 못해 고갈됨**

```
세일 시작 (09:00)
    │
    ▼
동시 요청 2,800건 (평소 200건)
    │
    ▼
Tomcat 스레드 200개 모두 사용 중
    │
    ├─ 신규 요청 → accept-count 대기열 누적
    │
    ▼
대기열 100건 초과
    │
    ▼
OS 레벨에서 연결 거부 (connection reset)
    │
    ▼
전체 서비스 중단
```

### 3.2 근본 원인

| 구분 | 원인 | 상세 |
|------|------|------|
| **설정** | 기본값 사용 | Tomcat, HikariCP 모두 프로덕션에 부적합한 기본값 |
| **테스트** | 부하 테스트 미흡 | 세일 규모 트래픽 사전 시뮬레이션 없음 |
| **모니터링** | 임계치 부재 | 스레드/커넥션 풀 사용률 알림 없음 |
| **대응** | 플레이북 부재 | 트래픽 폭증 시 대응 절차 미수립 |

### 3.3 5 Whys 분석

```
Q1: 왜 서버가 다운되었나?
A1: Tomcat 스레드 풀이 고갈되어 신규 연결을 거부했다.

Q2: 왜 스레드 풀이 고갈되었나?
A2: 동시 요청 2,800건을 처리할 수 없는 200개 한계였다.

Q3: 왜 200개로 설정되어 있었나?
A3: Spring Boot 기본값을 그대로 사용했다.

Q4: 왜 기본값을 변경하지 않았나?
A4: 실제 트래픽 규모를 예측한 부하 테스트를 하지 않았다.

Q5: 왜 부하 테스트를 하지 않았나?
A5: 세일 이벤트 전 성능 검증 프로세스가 없었다.
```

**근본 원인**: 이벤트 전 성능 검증 프로세스 부재 및 프로덕션 환경에 적합한 설정 가이드라인 미수립

---

## 4. 영향 및 피해 평가

### 4.1 서비스 영향

| 항목 | 수치 |
|------|------|
| 장애 시간 | 47분 |
| 영향받은 요청 수 | 약 125,000건 |
| 실패한 주문 시도 | 약 8,500건 |
| 영향받은 고유 사용자 | 약 15,000명 |

### 4.2 비즈니스 영향

| 항목 | 추정 피해 |
|------|----------|
| 직접 매출 손실 | 약 4,200만원 (평균 주문 5,000원 × 8,500건) |
| 쿠폰 보상 비용 | 약 750만원 (5,000원 쿠폰 × 15,000명) |
| 고객 신뢰도 | 측정 불가 (SNS 부정 언급 47건) |

### 4.3 기술 영향

- 모니터링 시스템: 정상 동작 (장애 탐지 성공)
- 데이터 정합성: 손실 없음 (트랜잭션 롤백 정상 처리)
- 인프라: MySQL, Redis 정상 동작

---

## 5. 문제 해결을 위한 즉시 조치 항목

### 5.1 수행된 조치

| 시간 | 조치 | 결과 |
|------|------|------|
| 09:15 | 서버 재시작 | 실패 (즉시 재발) |
| 09:20 | Nginx rate limiting 적용 | 부분 성공 (에러율 45%로 감소) |
| 09:30 | Tomcat/HikariCP 설정 튜닝 | 성공 |
| 09:47 | 서비스 정상화 확인 | 완료 |

### 5.2 적용된 설정 변경

```yaml
# 변경 전 (기본값)
server.tomcat.threads.max: 200
spring.datasource.hikari.maximum-pool-size: 10

# 변경 후
server.tomcat.threads.max: 400
server.tomcat.accept-count: 200
spring.datasource.hikari.maximum-pool-size: 50
spring.datasource.hikari.connection-timeout: 5000
```

---

## 6. 재발 방지를 위한 조치 항목

### 6.1 단기 조치 (1주 이내)

| 우선순위 | 항목 | 담당 | 상태 |
|----------|------|------|------|
| P0 | 프로덕션 서버 설정 튜닝 영구 적용 | 인프라팀 | ✅ 완료 |
| P0 | 스레드/커넥션 풀 사용률 알림 추가 | 모니터링팀 | ✅ 완료 |
| P1 | 트래픽 폭증 대응 플레이북 작성 | SRE팀 | 🔄 진행중 |
| P1 | Rate Limiting 상시 적용 검토 | 백엔드팀 | 📋 예정 |

### 6.2 중기 조치 (1개월 이내)

| 우선순위 | 항목 | 담당 | 상태 |
|----------|------|------|------|
| P1 | 대규모 이벤트 전 부하 테스트 의무화 | QA팀 | 📋 예정 |
| P1 | Auto-scaling 인프라 구축 | 인프라팀 | 📋 예정 |
| P2 | Circuit Breaker 패턴 적용 | 백엔드팀 | 📋 예정 |
| P2 | 장애 시뮬레이션 훈련 (분기별) | SRE팀 | 📋 예정 |

### 6.3 장기 조치 (분기 내)

| 항목 | 설명 |
|------|------|
| 성능 테스트 파이프라인 | CI/CD에 부하 테스트 자동화 통합 |
| 다중 인스턴스 아키텍처 | 단일 장애점 제거를 위한 수평 확장 |
| 카오스 엔지니어링 도입 | 정기적 장애 주입 테스트 |

---

## 7. 교훈 (Lessons Learned)

### 7.1 잘된 점

1. **모니터링 시스템이 정상 동작**: 장애 발생 3분 만에 알림 수신
2. **데이터 무결성 유지**: 트랜잭션 롤백이 정상 처리되어 데이터 손실 없음
3. **팀 협업**: 개발/인프라/CS팀이 신속하게 협력하여 47분 내 복구

### 7.2 개선이 필요한 점

1. **사전 준비 부족**: 세일 이벤트 규모에 맞는 부하 테스트 미실시
2. **기본값 의존**: 프로덕션 환경에 기본값을 그대로 사용
3. **플레이북 부재**: 트래픽 폭증 시 즉각 실행할 수 있는 대응 절차 없음

### 7.3 팀에 공유할 인사이트

> "기본값은 개발 환경을 위한 것이다. 프로덕션에서는 반드시 우리 서비스의 트래픽 패턴에 맞게 튜닝해야 한다."

> "장애는 피할 수 없지만, 같은 장애를 두 번 겪는 것은 피할 수 있다."

---

# 장애 보고서 #2: API 엔드포인트 불일치로 인한 기능 장애

## 1. 사건 개요

| 항목 | 내용 |
|------|------|
| **장애 등급** | P2 (High) |
| **발생 일시** | 2025-12-25 14:00 KST |
| **복구 일시** | 2025-12-25 15:30 KST |
| **총 장애 시간** | 90분 |
| **영향 서비스** | 인기 상품 조회, 포인트 충전 |
| **영향 사용자** | 약 3,000명 |

### 1.1 한 줄 요약

프론트엔드 배포 시 API 엔드포인트 경로가 변경된 백엔드와 불일치하여 인기 상품 조회와 포인트 충전 기능이 90분간 100% 실패했다.

---

## 2. 타임라인

```
2025-12-25

13:50  백엔드 v2.3.0 배포 완료
       ├─ 변경사항: API 경로 RESTful 규칙 통일
       │   ├─ /api/products/popular → /api/products/top
       │   └─ /api/points/charge → /api/points/users/{userId}/charge
       └─ 배포 검증: 단위 테스트 통과

14:00  프론트엔드 배포 시작 (변경 없음)
       └─ 기존 API 경로 그대로 사용

14:05  첫 번째 에러 로그 발생
       ├─ [ERROR] 404 Not Found: /api/products/popular
       └─ 모니터링 알림 미발생 (404는 경고 대상 아님)

14:30  고객 문의 접수 시작
       ├─ "인기 상품이 안 보여요"
       ├─ "포인트 충전이 안 돼요"
       └─ CS팀 → 개발팀 전달

14:35  개발팀 조사 시작
       ├─ 프론트엔드 네트워크 탭 확인
       └─ 404 에러 다수 발견

14:45  원인 파악 완료
       └─ 백엔드 API 경로 변경 vs 프론트엔드 미반영

15:00  핫픽스 결정
       ├─ 옵션 A: 프론트엔드 수정 배포 (30분 예상)
       ├─ 옵션 B: 백엔드 호환성 레이어 추가 (20분 예상)
       └─ 옵션 B 선택 (더 빠른 복구)

15:15  백엔드 호환성 레이어 배포
       └─ 기존 경로 → 신규 경로 리다이렉트 추가

15:25  서비스 정상화 확인
       └─ 404 에러 0건

15:30  장애 종료 선언
       └─ 프론트엔드 정식 수정은 다음 배포로 예약
```

---

## 3. 근본 원인 분석

### 3.1 직접 원인

백엔드 API 경로 변경 시 프론트엔드 팀과 사전 협의 없이 배포하여 엔드포인트 불일치 발생

### 3.2 근본 원인

| 구분 | 원인 |
|------|------|
| **커뮤니케이션** | API 변경 사항이 프론트엔드 팀에 공유되지 않음 |
| **프로세스** | API 스펙 변경 시 영향 분석 절차 없음 |
| **테스트** | E2E 테스트가 배포 파이프라인에 없음 |
| **모니터링** | 404 에러에 대한 알림 정책 없음 |

### 3.3 5 Whys 분석

```
Q1: 왜 인기 상품 조회가 실패했나?
A1: 프론트엔드가 존재하지 않는 API 경로를 호출했다.

Q2: 왜 존재하지 않는 경로를 호출했나?
A2: 백엔드 API 경로가 변경되었는데 프론트엔드가 이를 반영하지 않았다.

Q3: 왜 프론트엔드가 반영하지 않았나?
A3: API 변경 사항이 프론트엔드 팀에 공유되지 않았다.

Q4: 왜 공유되지 않았나?
A4: API 변경 시 영향받는 팀에 통보하는 프로세스가 없었다.

Q5: 왜 프로세스가 없었나?
A5: 팀 간 협업 절차가 명문화되어 있지 않았다.
```

**근본 원인**: API 변경 관리 프로세스 부재

---

## 4. 영향 및 피해 평가

| 항목 | 수치 |
|------|------|
| 장애 시간 | 90분 |
| 실패한 API 요청 | 약 45,000건 |
| 영향받은 사용자 | 약 3,000명 |
| 포인트 충전 실패 건수 | 약 500건 |

---

## 5. 즉시 조치 항목

| 조치 | 내용 |
|------|------|
| 호환성 레이어 추가 | 기존 경로를 신규 경로로 리다이렉트하는 컨트롤러 추가 |
| Deprecated 헤더 추가 | 기존 경로 호출 시 `Deprecated: true` 헤더 반환 |
| 프론트엔드 수정 예약 | 다음 정기 배포 시 신규 경로로 변경 |

---

## 6. 재발 방지 조치

### 6.1 프로세스 개선

| 항목 | 내용 |
|------|------|
| API 변경 리뷰 | 모든 API 경로/스펙 변경 시 프론트엔드 팀 리뷰 필수 |
| API 버저닝 | `/api/v1/`, `/api/v2/` 형태로 버전 관리 도입 |
| 변경 공지 채널 | Slack #api-changes 채널에 자동 알림 설정 |

### 6.2 테스트 강화

| 항목 | 내용 |
|------|------|
| E2E 테스트 | 배포 파이프라인에 Cypress/Playwright E2E 테스트 추가 |
| 계약 테스트 | Pact 등 Consumer-Driven Contract 테스트 도입 검토 |
| 스모크 테스트 | 배포 후 주요 API 호출 자동 검증 |

### 6.3 모니터링 개선

| 항목 | 내용 |
|------|------|
| 404 알림 | 404 에러 비율 1% 초과 시 알림 발생 |
| API 호출 대시보드 | 엔드포인트별 성공/실패율 실시간 모니터링 |

---

## 7. 교훈

### 7.1 잘된 점

1. 고객 문의를 통한 빠른 장애 인지 (CS팀 협력)
2. 원인 파악 후 10분 내 핫픽스 배포

### 7.2 개선이 필요한 점

1. API 변경 시 영향 분석 및 공유 프로세스 부재
2. E2E 테스트 부재로 통합 오류 미탐지
3. 404 에러에 대한 모니터링 부재

### 7.3 팀에 공유할 인사이트

> "API는 계약이다. 계약 변경은 당사자 모두의 동의가 필요하다."

> "단위 테스트 통과는 서비스가 동작한다는 증거가 아니다. E2E 테스트가 그 증거다."

---

# 부록: 장애 대응 체크리스트

## A. 장애 발생 시

```
□ 1. 장애 인지 및 등급 판정 (P1~P4)
□ 2. 담당자 호출 및 전파
     - P1/P2: 즉시 (Slack + 전화)
     - P3/P4: 업무시간 내
□ 3. 초기 상태 기록
     - 스크린샷
     - 에러 로그
     - 메트릭 그래프
□ 4. 영향 범위 파악
     - 영향받는 서비스
     - 영향받는 사용자 수
□ 5. 고객 공지 (P1/P2)
□ 6. 임시 조치 수행
□ 7. 서비스 정상화 확인
□ 8. 장애 종료 선언
□ 9. 포스트 모템 작성 (48시간 이내)
□ 10. 재발 방지 조치 추적
```

## B. 포스트 모템 템플릿

```markdown
# 장애 보고서: [제목]

## 1. 사건 개요
- 장애 등급:
- 발생 일시:
- 복구 일시:
- 총 장애 시간:
- 영향 서비스:
- 영향 사용자:

## 2. 타임라인
(시간순 상세 기록)

## 3. 근본 원인 분석
- 직접 원인:
- 근본 원인:
- 5 Whys:

## 4. 영향 및 피해 평가
- 서비스 영향:
- 비즈니스 영향:

## 5. 즉시 조치 항목
(수행된 조치와 결과)

## 6. 재발 방지 조치
- 단기 (1주):
- 중기 (1개월):
- 장기 (분기):

## 7. 교훈
- 잘된 점:
- 개선 필요:
- 팀 공유 인사이트:

---
작성자:
작성일:
리뷰어:
```

## C. 장애 등급 정의

| 등급 | 명칭 | 정의 | 대응 시간 | 예시 |
|------|------|------|----------|------|
| P1 | Critical | 서비스 전체 중단 | 15분 이내 | 서버 다운, DB 접속 불가 |
| P2 | High | 핵심 기능 장애 | 30분 이내 | 주문 불가, 결제 실패 |
| P3 | Medium | 부분 기능 장애 | 2시간 이내 | 인기 상품 조회 실패 |
| P4 | Low | 사소한 이슈 | 24시간 이내 | 로그 누락, UI 깨짐 |

## D. 비상 연락처

| 역할 | 담당 | 연락처 | 담당 영역 |
|------|------|--------|----------|
| Primary On-Call | TBD | TBD | 전체 시스템 |
| DB Admin | TBD | TBD | MySQL |
| Infra | TBD | TBD | 서버, Redis, Kafka |
| Backend Lead | TBD | TBD | 애플리케이션 |

---

## 참고 자료

- [GitHub October 21 Post-Incident Analysis](https://github.blog/news-insights/company-news/oct21-post-incident-analysis/)
- [LINE 장애 보고 및 후속 조치 프로세스 문화](https://engineering.linecorp.com/ko/blog/line-failure-reporting-and-follow-up-process-culture)
- [Google SRE - Postmortem Culture](https://sre.google/sre-book/postmortem-culture/)
